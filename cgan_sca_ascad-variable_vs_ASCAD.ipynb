{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from os.path import exists\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.stats import entropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from src.datasets.load_ascadr import *\n",
    "from src.datasets.load_ascadf import *\n",
    "from src.datasets.load_dpav42 import *\n",
    "from src.datasets.load_eshard import *\n",
    "from src.datasets.load_chesctf import *\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_root_path = \"D:/traces\"\n",
    "results_root_path = \"D:/postdoc/paper_cgan_features\"\n",
    "features_root_path = \"D:/postdoc/paper_cgan_features/cgan_reference_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_reference_name = \"ascad-variable\"\n",
    "dataset_target_name = \"ASCAD\"\n",
    "dataset_reference_path = f\"{dataset_root_path}/ASCADr/ascad-variable_nopoi/ascad-variable_nopoi_window_20.h5\"\n",
    "dataset_target_path = f\"{dataset_root_path}/ASCADf/ASCAD_nopoi/ASCAD_nopoi_window_20.h5\"\n",
    "dataset_reference_dim = 25000\n",
    "dataset_target_dim = 10000\n",
    "n_profiling_reference = 200000\n",
    "n_attack_reference = 5000\n",
    "n_profiling_target = 50000\n",
    "n_validation_target = 5000\n",
    "n_attack_target = 5000\n",
    "n_attack_ge = 2000\n",
    "target_byte_reference = 2\n",
    "target_byte_target = 2\n",
    "leakage_model = \"ID\"\n",
    "cgan_epochs = 100\n",
    "batch_size = 400\n",
    "cgan_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def snr_fast(x, y):\n",
    "    ns = x.shape[1]\n",
    "    unique = np.unique(y)\n",
    "    means = np.zeros((len(unique), ns))\n",
    "    variances = np.zeros((len(unique), ns))\n",
    "\n",
    "    for i, u in enumerate(unique):\n",
    "        new_x = x[np.argwhere(y == int(u))]\n",
    "        means[i] = np.mean(new_x, axis=0)\n",
    "        variances[i] = np.var(new_x, axis=0)\n",
    "    return np.var(means, axis=0) / np.mean(variances, axis=0)\n",
    "\n",
    "\n",
    "def get_features(dataset, target_byte: int, n_poi=100):\n",
    "    snr_val_share_1 = snr_fast(np.array(dataset.x_profiling, dtype=np.int16), np.asarray(dataset.share1_profiling[target_byte, :]))\n",
    "    snr_val_share_2 = snr_fast(np.array(dataset.x_profiling, dtype=np.int16), np.asarray(dataset.share2_profiling[target_byte, :]))\n",
    "    snr_val_share_1[np.isnan(snr_val_share_1)] = 0\n",
    "    snr_val_share_2[np.isnan(snr_val_share_2)] = 0\n",
    "    ind_snr_masks_poi_sm = np.argsort(snr_val_share_1)[::-1][:int(n_poi / 2)]\n",
    "    ind_snr_masks_poi_sm_sorted = np.sort(ind_snr_masks_poi_sm)\n",
    "    ind_snr_masks_poi_r2 = np.argsort(snr_val_share_2)[::-1][:int(n_poi / 2)]\n",
    "    ind_snr_masks_poi_r2_sorted = np.sort(ind_snr_masks_poi_r2)\n",
    "\n",
    "    poi_profiling = np.concatenate((ind_snr_masks_poi_sm_sorted, ind_snr_masks_poi_r2_sorted), axis=0)\n",
    "\n",
    "    snr_val_share_1 = snr_fast(np.array(dataset.x_attack, dtype=np.int16), np.asarray(dataset.share1_attack[target_byte, :]))\n",
    "    snr_val_share_2 = snr_fast(np.array(dataset.x_attack, dtype=np.int16), np.asarray(dataset.share2_attack[target_byte, :]))\n",
    "    snr_val_share_1[np.isnan(snr_val_share_1)] = 0\n",
    "    snr_val_share_2[np.isnan(snr_val_share_2)] = 0\n",
    "    ind_snr_masks_poi_sm = np.argsort(snr_val_share_1)[::-1][:int(n_poi / 2)]\n",
    "    ind_snr_masks_poi_sm_sorted = np.sort(ind_snr_masks_poi_sm)\n",
    "    ind_snr_masks_poi_r2 = np.argsort(snr_val_share_2)[::-1][:int(n_poi / 2)]\n",
    "    ind_snr_masks_poi_r2_sorted = np.sort(ind_snr_masks_poi_r2)\n",
    "\n",
    "    poi_attack = np.concatenate((ind_snr_masks_poi_sm_sorted, ind_snr_masks_poi_r2_sorted), axis=0)\n",
    "\n",
    "    return dataset.x_profiling[:, poi_profiling], dataset.x_attack[:, poi_attack]\n",
    "\n",
    "\n",
    "def load_dataset(identifier, dataset_file, target_byte, traces_dim, n_prof, n_val, n_attack, reference=False):\n",
    "    implement_reference_feature_selection = False\n",
    "    reference_features_shortcut = \"\"\n",
    "    if reference:\n",
    "        \"\"\" If features were already computed for this dataset, target key byte,\n",
    "        and leakage model, there is no need to compute it again\"\"\"\n",
    "        reference_features_shortcut = f'{features_root_path}/selected_{cgan_features}_features_snr_{dataset_reference_name}_{dataset_reference_dim}_target_byte_{target_byte_reference}.h5'\n",
    "        if exists(reference_features_shortcut):\n",
    "            print(\"Reference features already created.\")\n",
    "            dataset_file = reference_features_shortcut\n",
    "            traces_dim = cgan_features\n",
    "        else:\n",
    "            implement_reference_feature_selection = True\n",
    "\n",
    "    dataset = None\n",
    "    if identifier == \"ascad-variable\":\n",
    "        dataset = ReadASCADr(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=traces_dim)\n",
    "    if identifier == \"ASCAD\":\n",
    "        dataset = ReadASCADf(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=traces_dim)\n",
    "    if identifier == \"eshard\":\n",
    "        dataset = ReadEshard(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=traces_dim)\n",
    "    if identifier == \"dpa_v42\":\n",
    "        dataset = ReadDPAV42(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=traces_dim)\n",
    "    if identifier == \"ches_ctf\":\n",
    "        dataset = ReadCHESCTF(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=traces_dim)\n",
    "\n",
    "    if implement_reference_feature_selection:\n",
    "        generate_features_h5(dataset, target_byte, reference_features_shortcut, cgan_features)\n",
    "        dataset_file = reference_features_shortcut\n",
    "\n",
    "        if identifier == \"ascad-variable\":\n",
    "            return ReadASCADr(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=cgan_features)\n",
    "        if identifier == \"ASCAD\":\n",
    "            return ReadASCADf(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=cgan_features)\n",
    "        if identifier == \"eshard\":\n",
    "            return ReadEshard(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=cgan_features)\n",
    "        if identifier == \"dpa_v42\":\n",
    "            return ReadDPAV42(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=cgan_features)\n",
    "        if identifier == \"ches_ctf\":\n",
    "            return ReadCHESCTF(n_prof, n_val, n_attack, target_byte, leakage_model, dataset_file, number_of_samples=cgan_features)\n",
    "    else:\n",
    "        return dataset\n",
    "\n",
    "\n",
    "def scale_dataset(prof_set, attack_set, scaler):\n",
    "    prof_new = scaler.fit_transform(prof_set)\n",
    "    if attack_set is not None:\n",
    "        attack_new = scaler.transform(attack_set)\n",
    "    else:\n",
    "        attack_new = None\n",
    "    return prof_new, attack_new\n",
    "\n",
    "\n",
    "def generate_features_h5(dataset, target_byte, save_file_path, num_features):\n",
    "    profiling_traces_rpoi, attack_traces_rpoi = get_features(dataset, target_byte, num_features)\n",
    "    out_file = h5py.File(save_file_path, 'w')\n",
    "\n",
    "    profiling_index = [n for n in range(dataset.n_profiling)]\n",
    "    attack_index = [n for n in range(dataset.n_attack)]\n",
    "\n",
    "    profiling_traces_group = out_file.create_group(\"Profiling_traces\")\n",
    "    attack_traces_group = out_file.create_group(\"Attack_traces\")\n",
    "\n",
    "    profiling_traces_group.create_dataset(name=\"traces\", data=profiling_traces_rpoi, dtype=profiling_traces_rpoi.dtype)\n",
    "    attack_traces_group.create_dataset(name=\"traces\", data=attack_traces_rpoi, dtype=attack_traces_rpoi.dtype)\n",
    "\n",
    "    metadata_type_profiling = np.dtype([(\"plaintext\", dataset.profiling_plaintexts.dtype, (len(dataset.profiling_plaintexts[0]),)),\n",
    "                                        (\"key\", dataset.profiling_keys.dtype, (len(dataset.profiling_keys[0]),)),\n",
    "                                        (\"masks\", dataset.profiling_masks.dtype, (len(dataset.profiling_masks[0]),))\n",
    "                                        ])\n",
    "    metadata_type_attack = np.dtype([(\"plaintext\", dataset.attack_plaintexts.dtype, (len(dataset.attack_plaintexts[0]),)),\n",
    "                                     (\"key\", dataset.attack_keys.dtype, (len(dataset.attack_keys[0]),)),\n",
    "                                     (\"masks\", dataset.attack_masks.dtype, (len(dataset.attack_masks[0]),))\n",
    "                                     ])\n",
    "\n",
    "    profiling_metadata = np.array(\n",
    "        [(dataset.profiling_plaintexts[n], dataset.profiling_keys[n], dataset.profiling_masks[n]) for n in profiling_index],\n",
    "        dtype=metadata_type_profiling)\n",
    "    profiling_traces_group.create_dataset(\"metadata\", data=profiling_metadata, dtype=metadata_type_profiling)\n",
    "\n",
    "    attack_metadata = np.array([(dataset.attack_plaintexts[n], dataset.attack_keys[n], dataset.attack_masks[n]) for n in attack_index],\n",
    "                               dtype=metadata_type_attack)\n",
    "    attack_traces_group.create_dataset(\"metadata\", data=attack_metadata, dtype=metadata_type_attack)\n",
    "\n",
    "    out_file.flush()\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference features already created.\n"
     ]
    }
   ],
   "source": [
    "dataset_reference = load_dataset(dataset_reference_name, dataset_reference_path, target_byte_reference, dataset_reference_dim,\n",
    "                                 n_profiling_reference, 0, n_attack_reference, reference=True)\n",
    "dataset_target = load_dataset(dataset_target_name, dataset_target_path, target_byte_target, dataset_target_dim,\n",
    "                              n_profiling_target, n_validation_target, n_attack_target)\n",
    "\n",
    "dataset_reference.x_profiling, dataset_reference.x_attack = scale_dataset(dataset_reference.x_profiling, dataset_reference.x_attack,\n",
    "                                                                          StandardScaler())\n",
    "dataset_target.x_profiling, dataset_target.x_attack = scale_dataset(dataset_target.x_profiling, dataset_target.x_attack, StandardScaler())\n",
    "\n",
    "features_reference_profiling, features_reference_attack = dataset_reference.x_profiling, dataset_reference.x_attack\n",
    "\n",
    "\"\"\" the following is used only for verification, not in the CGAN training \"\"\"\n",
    "features_target_profiling, features_target_attack = get_features(dataset_target, target_byte_target, n_poi=cgan_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def best_models_random_search(reference, target):\n",
    "    hp_disc, hp_gen = {}, {}\n",
    "    if reference == \"ascad-variable\":\n",
    "        if target == \"ASCAD\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 100, 'neurons_dropout': 200, 'layers_embed': 2, 'layers_dropout': 3, 'dropout': 0.7\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 300, 'layers': 1, 'activation': 'linear'\n",
    "            }\n",
    "        if target == \"dpa_v42\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 100, 'neurons_dropout': 200, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.8\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 200, 'layers': 4, 'activation': 'linear', 'neurons_2': 200, 'neurons_3': 200, 'neurons_4': 100\n",
    "            }\n",
    "        if target == \"ches_ctf\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 200, 'neurons_dropout': 200, 'layers_embed': 2, 'layers_dropout': 1, 'dropout': 0.5\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 100, 'layers': 4, 'activation': 'linear', 'neurons_2': 100, 'neurons_3': 100, 'neurons_4': 100\n",
    "            }\n",
    "        if target == \"eshard\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 200, 'neurons_dropout': 200, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.7\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 500, 'layers': 3, 'activation': 'leakyrelu', 'neurons_2': 500, 'neurons_3': 100\n",
    "            }\n",
    "    if reference == \"ASCAD\":\n",
    "        if target == \"ascad-variable\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 200, 'neurons_dropout': 200, 'layers_embed': 2, 'layers_dropout': 1, 'dropout': 0.6,\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 200, 'layers': 3, 'activation': 'leakyrelu', 'neurons_2': 200, 'neurons_3': 100\n",
    "            }\n",
    "        if target == \"dpa_v42\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 200, 'neurons_dropout': 100, 'layers_embed': 2, 'layers_dropout': 2, 'dropout': 0.8\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 300, 'layers': 2, 'activation': 'linear', 'neurons_2': 100\n",
    "            }\n",
    "        if target == \"ches_ctf\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 200, 'neurons_dropout': 200, 'layers_embed': 2, 'layers_dropout': 1, 'dropout': 0.5\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 100, 'layers': 4, 'activation': 'linear', 'neurons_2': 100, 'neurons_3': 100, 'neurons_4': 100\n",
    "            }\n",
    "        if target == \"eshard\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 500, 'neurons_dropout': 200, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.7\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 500, 'layers': 2, 'activation': 'selu', 'neurons_2': 400\n",
    "            }\n",
    "    if reference == \"dpa_v42\":\n",
    "        if target == \"ascad-variable\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 500, 'neurons_dropout': 100, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.6\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 100, 'layers': 1, 'activation': 'elu'\n",
    "            }\n",
    "        if target == \"ASCAD\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 100, 'neurons_dropout': 200, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.7\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 500, 'layers': 4, 'activation': 'linear', 'neurons_2': 100, 'neurons_3': 100, 'neurons_4': 100\n",
    "            }\n",
    "        if target == \"ches_ctf\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 500, 'neurons_dropout': 500, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.8\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 100, 'layers': 1, 'activation': 'linear'\n",
    "            }\n",
    "        if target == \"eshard\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 500, 'neurons_dropout': 500, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.6\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 400, 'layers': 2, 'activation': 'selu', 'neurons_2': 300\n",
    "            }\n",
    "    if reference == \"eshard\":\n",
    "        if target == \"ascad-variable\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 200, 'neurons_dropout': 100, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.7\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 100, 'layers': 4, 'activation': 'linear', 'neurons_2': 100, 'neurons_3': 100, 'neurons_4': 100\n",
    "            }\n",
    "        if target == \"ASCAD\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 100, 'neurons_dropout': 500, 'layers_embed': 1, 'layers_dropout': 1, 'dropout': 0.6\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 400, 'layers': 2, 'activation': 'linear', 'neurons_2': 300\n",
    "            }\n",
    "        if target == \"dpa_v42\":\n",
    "            hp_disc = {\n",
    "                'neurons_embed': 100, 'neurons_dropout': 500, 'layers_embed': 2, 'layers_dropout': 2, 'dropout': 0.8\n",
    "            }\n",
    "            hp_gen = {\n",
    "                'neurons_1': 500, 'layers': 1, 'activation': 'linear'\n",
    "            }\n",
    "    return hp_disc, hp_gen\n",
    "\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = cross_entropy_disc(tf.ones_like(real), real)\n",
    "    fake_loss = cross_entropy_disc(tf.zeros_like(fake), fake)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return cross_entropy(tf.ones_like(fake), fake)\n",
    "\n",
    "\n",
    "def define_discriminator(features_dim, n_classes, hp_disc):\n",
    "    # label input\n",
    "    in_label = Input(shape=1)\n",
    "    y = Embedding(n_classes, n_classes)(in_label)\n",
    "    for l_i in range(hp_disc[\"layers_embed\"]):\n",
    "        y = Dense(hp_disc[\"neurons_embed\"], kernel_initializer='random_normal')(y)\n",
    "        y = LeakyReLU()(y)\n",
    "    y = Flatten()(y)\n",
    "\n",
    "    in_features = Input(shape=(features_dim,))\n",
    "\n",
    "    merge = Concatenate()([y, in_features])\n",
    "\n",
    "    x = None\n",
    "    for l_i in range(hp_disc[\"layers_dropout\"]):\n",
    "        x = Dense(hp_disc[\"neurons_dropout\"], kernel_initializer='random_normal')(merge if l_i == 0 else x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dropout(hp_disc[\"dropout\"])(x)\n",
    "\n",
    "    # output\n",
    "    out_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model([in_label, in_features], out_layer)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "# define a random generator model\n",
    "def define_generator(input_dim, output_dim, hp_gen):\n",
    "    in_traces = Input(shape=(input_dim,))\n",
    "    x = None\n",
    "    for l_i in range(hp_gen[\"layers\"]):\n",
    "        x = Dense(hp_gen[f\"neurons_{l_i + 1}\"],\n",
    "                  activation=hp_gen[\"activation\"] if hp_gen[\"activation\"] != \"leakyrelu\" else None)(in_traces if l_i == 0 else x)\n",
    "        if hp_gen[\"activation\"] == \"leakyrelu\":\n",
    "            x = LeakyReLU()(x)\n",
    "    out_layer = Dense(output_dim, activation='linear')(x)\n",
    "\n",
    "    model = Model([in_traces], out_layer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "real_accuracy_metric = BinaryAccuracy()\n",
    "fake_accuracy_metric = BinaryAccuracy()\n",
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "cross_entropy_disc = BinaryCrossentropy(from_logits=True)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0025, beta_1=0.5)\n",
    "\n",
    "classes = 9 if leakage_model == \"HW\" else 256\n",
    "\n",
    "hp_d, hp_g = best_models_random_search(dataset_reference_name, dataset_target_name)\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator(cgan_features, classes, hp_d)\n",
    "# create the generator\n",
    "generator = define_generator(dataset_target_dim, cgan_features, hp_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Metrics to assess quality of profiling attack: Max SNR, Guessing entropy, Ntraces_GE = 1, Perceived Information \"\"\"\n",
    "max_snr_share_1 = []\n",
    "max_snr_share_2 = []\n",
    "\n",
    "ge_cgansca = []\n",
    "nt_cgansca = []\n",
    "pi_cgansca = []\n",
    "\n",
    "\"\"\" Just for plot \"\"\"\n",
    "x_axis_epochs = []\n",
    "\n",
    "\"\"\" Accuracy for real and synthetic data \"\"\"\n",
    "real_acc = []\n",
    "fake_acc = []\n",
    "\n",
    "\"\"\" Generator and Discriminator Losses \"\"\"\n",
    "g_loss = []\n",
    "d_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_reference_samples(bs):\n",
    "    rnd = np.random.randint(0, dataset_reference.n_profiling - bs)\n",
    "    features = features_reference_profiling[rnd:rnd + bs]\n",
    "    labels = dataset_reference.profiling_labels[rnd:rnd + bs]\n",
    "    return [features, labels]\n",
    "\n",
    "\n",
    "def generate_target_samples(bs):\n",
    "    rnd = np.random.randint(0, dataset_target.n_profiling - bs)\n",
    "    traces = dataset_target.x_profiling[rnd:rnd + bs]\n",
    "    labels = dataset_target.profiling_labels[rnd:rnd + bs]\n",
    "    return [traces, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(traces_batch, label_traces, features, label_features):\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_features = generator(traces_batch)\n",
    "        real_output = discriminator([label_features, features])\n",
    "        fake_output = discriminator([label_traces, fake_features])\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_features = generator(traces_batch)\n",
    "        fake_output = discriminator([label_traces, fake_features])\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    fake_accuracy_metric.update_state(tf.zeros_like(fake_features), fake_output)\n",
    "    real_accuracy_metric.update_state(tf.ones_like(features), real_output)\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "\n",
    "def compute_snr_reference_features():\n",
    "    batch_size_reference = 10000\n",
    "\n",
    "    # prepare traces from target dataset\n",
    "    rnd_reference = random.randint(0, len(dataset_reference.x_profiling) - batch_size_reference)\n",
    "    features_reference = features_reference_profiling[rnd_reference:rnd_reference + batch_size_reference]\n",
    "\n",
    "    snr_reference_features_share_1 = snr_fast(features_reference,\n",
    "                                              dataset_reference.share1_profiling[target_byte_reference,\n",
    "                                              rnd_reference:rnd_reference + batch_size_reference])\n",
    "    snr_reference_features_share_2 = snr_fast(features_reference,\n",
    "                                              dataset_reference.share2_profiling[target_byte_reference,\n",
    "                                              rnd_reference:rnd_reference + batch_size_reference])\n",
    "    plt.title(\"$f_{ref}$\")\n",
    "    plt.plot(snr_reference_features_share_1, label=\"share 1\")\n",
    "    plt.plot(snr_reference_features_share_2, label=\"share 2\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"SNR\")\n",
    "    plt.xlim([1, cgan_features])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def compute_snr_target_features(epoch, epoch_snr_step, synthetic_traces=True):\n",
    "    batch_size_target = 10000\n",
    "\n",
    "    # prepare traces from target dataset\n",
    "    rnd_target = random.randint(0, len(dataset_target.x_profiling) - batch_size_target)\n",
    "\n",
    "    if synthetic_traces:\n",
    "        traces_target = dataset_target.x_profiling[rnd_target:rnd_target + batch_size_target]\n",
    "        features_target = generator.predict([traces_target])\n",
    "    else:\n",
    "        features_target = features_target_profiling[rnd_target:rnd_target + batch_size_target]\n",
    "\n",
    "    snr_target_features_share_1 = snr_fast(features_target,\n",
    "                                           dataset_target.share1_profiling[target_byte_target,\n",
    "                                           rnd_target:rnd_target + batch_size_target]).tolist()\n",
    "    snr_target_features_share_2 = snr_fast(features_target,\n",
    "                                           dataset_target.share2_profiling[target_byte_target,\n",
    "                                           rnd_target:rnd_target + batch_size_target]).tolist()\n",
    "\n",
    "    if synthetic_traces:\n",
    "        if (epoch + 1) % epoch_snr_step == 0:\n",
    "            plt.plot(snr_target_features_share_1)\n",
    "            plt.plot(snr_target_features_share_2)\n",
    "            plt.xlim([1, cgan_features])\n",
    "            plt.title(\"$f_{target}$ (Synthetic)\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    else:\n",
    "        plt.plot(snr_target_features_share_1)\n",
    "        plt.plot(snr_target_features_share_2)\n",
    "        plt.xlim([1, cgan_features])\n",
    "        plt.title(\"$f_{target}$ (Real)\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    if synthetic_traces:\n",
    "        max_snr_share_1.append(np.max(snr_target_features_share_1))\n",
    "        max_snr_share_2.append(np.max(snr_target_features_share_2))\n",
    "        if (epoch + 1) % epoch_snr_step == 0:\n",
    "            plt.title(\"Max SNR $f_{target}$\")\n",
    "            plt.plot(max_snr_share_1, label=\"Max SNR Share 1\")\n",
    "            plt.plot(max_snr_share_2, label=\"Max SNR Share 2\")\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"SNR\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def mlp(output_dim, number_of_samples, learning_rate=0.001):\n",
    "    input_shape = (number_of_samples)\n",
    "    input_layer = Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "    x = Dense(100, kernel_initializer=\"glorot_normal\", activation=\"elu\")(input_layer)\n",
    "    x = Dense(100, kernel_initializer=\"glorot_normal\", activation=\"elu\")(x)\n",
    "    x = Dense(100, kernel_initializer=\"glorot_normal\", activation=\"elu\")(x)\n",
    "    x = Dense(100, kernel_initializer=\"glorot_normal\", activation=\"elu\")(x)\n",
    "\n",
    "    output_layer = Dense(output_dim, activation='softmax', name=f'output')(x)\n",
    "\n",
    "    m_model = Model(input_layer, output_layer, name='mlp_softmax')\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    m_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    m_model.summary()\n",
    "    return m_model\n",
    "\n",
    "\n",
    "def guessing_entropy(predictions, labels_guess, good_key, key_rank_attack_traces, key_rank_report_interval=1):\n",
    "    \"\"\"\n",
    "    Function to compute Guessing Entropy\n",
    "    - this function computes a list of key candidates, ordered by their probability of being the correct key\n",
    "    - if this function returns final_ge=1, it means that the correct key is actually indicated as the most likely one.\n",
    "    - if this function returns final_ge=256, it means that the correct key is actually indicated as the least likely one.\n",
    "    - if this function returns final_ge close to 128, it means that the attack is wrong and the model is simply returing a random key.\n",
    "\n",
    "    :return\n",
    "    - final_ge: the guessing entropy of the correct key\n",
    "    - guessing_entropy: a vector indicating the value 'final_ge' with respect to the number of processed attack measurements\n",
    "    - number_of_measurements_for_ge_1: the number of processed attack measurements necessary to reach final_ge = 1\n",
    "    \"\"\"\n",
    "\n",
    "    nt = len(predictions)\n",
    "\n",
    "    key_rank_executions = 40\n",
    "\n",
    "    # key_ranking_sum = np.zeros(key_rank_attack_traces)\n",
    "    key_ranking_sum = np.zeros(\n",
    "        int(key_rank_attack_traces / key_rank_report_interval))\n",
    "\n",
    "    predictions = np.log(predictions + 1e-36)\n",
    "\n",
    "    probabilities_kg_all_traces = np.zeros((nt, 256))\n",
    "    for index in range(nt):\n",
    "        probabilities_kg_all_traces[index] = predictions[index][\n",
    "            np.asarray([int(leakage[index])\n",
    "                        for leakage in labels_guess[:]])\n",
    "        ]\n",
    "\n",
    "    for run in range(key_rank_executions):\n",
    "        r = np.random.choice(\n",
    "            range(nt), key_rank_attack_traces, replace=False)\n",
    "        probabilities_kg_all_traces_shuffled = probabilities_kg_all_traces[r]\n",
    "        key_probabilities = np.zeros(256)\n",
    "\n",
    "        kr_count = 0\n",
    "        for index in range(key_rank_attack_traces):\n",
    "\n",
    "            key_probabilities += probabilities_kg_all_traces_shuffled[index]\n",
    "            key_probabilities_sorted = np.argsort(key_probabilities)[::-1]\n",
    "\n",
    "            if (index + 1) % key_rank_report_interval == 0:\n",
    "                key_ranking_good_key = list(\n",
    "                    key_probabilities_sorted).index(good_key) + 1\n",
    "                key_ranking_sum[kr_count] += key_ranking_good_key\n",
    "                kr_count += 1\n",
    "\n",
    "    ge = key_ranking_sum / key_rank_executions\n",
    "\n",
    "    number_of_measurements_for_ge_1 = key_rank_attack_traces\n",
    "    if ge[int(key_rank_attack_traces / key_rank_report_interval) - 1] < 2:\n",
    "        for index in range(int(key_rank_attack_traces / key_rank_report_interval) - 1, -1, -1):\n",
    "            if ge[index] > 2:\n",
    "                number_of_measurements_for_ge_1 = (index + 1) * key_rank_report_interval\n",
    "                break\n",
    "\n",
    "    final_ge = ge[int(key_rank_attack_traces / key_rank_report_interval) - 1]\n",
    "    print(\"GE = {}\".format(final_ge))\n",
    "    print(\"Number of traces to reach GE = 1: {}\".format(number_of_measurements_for_ge_1))\n",
    "\n",
    "    return final_ge, ge, number_of_measurements_for_ge_1\n",
    "\n",
    "\n",
    "def perceived_information(model, labels, num_classes):\n",
    "    labels = np.array(labels, dtype=np.uint8)\n",
    "    p_k = np.ones(num_classes, dtype=np.float64)\n",
    "    for k in range(num_classes):\n",
    "        p_k[k] = np.count_nonzero(labels == k)\n",
    "    p_k /= len(labels)\n",
    "\n",
    "    acc = entropy(p_k, base=2)  # we initialize the value with H(K)\n",
    "\n",
    "    y_pred = np.array(model + 1e-36)\n",
    "\n",
    "    for k in range(num_classes):\n",
    "        trace_index_with_label_k = np.where(labels == k)[0]\n",
    "        y_pred_k = y_pred[trace_index_with_label_k, k]\n",
    "\n",
    "        y_pred_k = np.array(y_pred_k)\n",
    "        if len(y_pred_k) > 0:\n",
    "            p_k_l = np.sum(np.log2(y_pred_k)) / len(y_pred_k)\n",
    "            acc += p_k[k] * p_k_l\n",
    "\n",
    "    print(f\"PI: {acc}\")\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def attack(dataset, generator_model, features_dim):\n",
    "    \"\"\" Generate a batch of synthetic measurements with the trained generator \"\"\"\n",
    "    features_target_attack_syn = np.array(generator_model.predict([dataset.x_attack]))\n",
    "    features_target_profiling_syn = np.array(generator_model.predict([dataset.x_profiling]))\n",
    "\n",
    "    \"\"\" Define a neural network (MLP) to be trained with synthetic traces \"\"\"\n",
    "    attack_model = mlp(dataset.classes, features_dim)\n",
    "    attack_model.fit(\n",
    "        x=features_target_profiling_syn,\n",
    "        y=to_categorical(dataset.profiling_labels, num_classes=dataset.classes),\n",
    "        batch_size=400,\n",
    "        verbose=2,\n",
    "        epochs=50,\n",
    "        shuffle=True,\n",
    "        validation_data=(\n",
    "            features_target_attack_syn, to_categorical(dataset.attack_labels, num_classes=dataset.classes)),\n",
    "        callbacks=[])\n",
    "\n",
    "    \"\"\" Predict the trained MLP with target/attack measurements \"\"\"\n",
    "    predictions = attack_model.predict(features_target_attack_syn)\n",
    "    \"\"\" Check if we are able to recover the key from the target/attack measurements \"\"\"\n",
    "    ge, ge_vector, nt = guessing_entropy(predictions, dataset.labels_key_hypothesis_attack, dataset.correct_key_attack, 2000)\n",
    "    pi = perceived_information(predictions, dataset.attack_labels, dataset.classes)\n",
    "    return ge, nt, pi, ge_vector\n",
    "\n",
    "\n",
    "def attack_eval(epoch):\n",
    "    ge, nt, pi, ge_vector = attack(dataset_target, generator, cgan_features)\n",
    "    ge_cgansca.append(ge)\n",
    "    nt_cgansca.append(nt)\n",
    "    pi_cgansca.append(pi)\n",
    "\n",
    "    plt.plot(ge_vector, label=\"CGAN-SCA\")\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Attack Traces\")\n",
    "    plt.ylabel(\"Guessing Entropy\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train():\n",
    "    training_set_size = max(dataset_reference.n_profiling, dataset_target.n_profiling)\n",
    "\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    n_batches = int(training_set_size / batch_size)\n",
    "\n",
    "    epoch_snr_step = 10\n",
    "\n",
    "    # manually enumerate epochs\n",
    "    for e in range(cgan_epochs):\n",
    "        for b in range(n_batches):\n",
    "            [features_reference, labels_reference] = generate_reference_samples(batch_size)\n",
    "            [traces_target, labels_target] = generate_target_samples(batch_size)\n",
    "\n",
    "            # Custom training step for speed and versatility\n",
    "            gen_loss, disc_loss = train_step(traces_target, labels_target, features_reference, labels_reference)\n",
    "\n",
    "            if (b + 1) % 100 == 0:\n",
    "                real_acc.append(real_accuracy_metric.result())\n",
    "                fake_acc.append(fake_accuracy_metric.result())\n",
    "                g_loss.append(gen_loss)\n",
    "                d_loss.append(disc_loss)\n",
    "\n",
    "                print(\n",
    "                    f\"epoch: {e}, batch: {b}, d_loss: {disc_loss}, g_loss: {gen_loss}, real_acc: {real_accuracy_metric.result()}, fake_acc: {fake_accuracy_metric.result()}\")\n",
    "\n",
    "        # Split eval steps up as attacking takes significant time while snr computation is fast\n",
    "        if e == 0:\n",
    "            compute_snr_reference_features()\n",
    "            compute_snr_target_features(e, epoch_snr_step, synthetic_traces=False)\n",
    "        else:\n",
    "            compute_snr_target_features(e, epoch_snr_step)\n",
    "        if (e + 1) % epoch_snr_step == 0:\n",
    "            plt.plot(real_acc, label=\"Real\")\n",
    "            plt.plot(fake_acc, label=\"Fake\")\n",
    "            plt.axhline(y=0.5, linestyle=\"dashed\", color=\"black\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(g_loss, label=\"g_loss\")\n",
    "            plt.plot(d_loss, label=\"d_Loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        if (e + 1) % 50 == 0:\n",
    "            attack_eval(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}